import numpy as np

import argparse
import pickle
import os
import shutil

import keras
from keras import backend as K

import utils
from datasets import get_data_generator



def nearest_centroid_softmax(x, centroids):
    
    centr = K.constant(centroids.T)
    centr_norm = K.constant((centroids.T ** 2).sum(axis = 0, keepdims = True))
    pred_norm = K.sum(K.square(x), axis = -1, keepdims = True)
    dist = pred_norm + centr_norm - 2 * K.dot(x, centr)

    return K.log(K.maximum(1e-7, K.softmax(K.mean(dist, axis = -1, keepdims = True) - dist)))


def gen_inputs(gen, embeddings):
    
    for X in gen:
        if isinstance(X, tuple):
            X, y = X
            yield (X, [keras.utils.to_categorical(y, embeddings[0].shape[0]), keras.utils.to_categorical(y, embeddings[0].shape[0])] + [embed[y] for embed in embeddings])
        else:
            yield X


class UnitL1Norm(keras.constraints.Constraint):

    def __init__(self, axis=0):
        self.axis = axis

    def __call__(self, w):
        return w / (K.epsilon() + K.sum(K.abs(w), axis=self.axis, keepdims=True))

    def get_config(self):
        return {'axis': self.axis}


class PrintWeightsCallback(keras.callbacks.Callback):
    
    def __init__(self, layer):
        super(PrintWeightsCallback, self).__init__()
        self.layer = layer
    
    def on_epoch_end(self, epoch, logs=None):
        print(self.model.get_layer(self.layer).get_weights())



if __name__ == '__main__':

    # Parse arguments
    parser = argparse.ArgumentParser(description = 'Learns to map Cifar-100 images onto class embeddings.', formatter_class = argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('--dataset', type = str, required = True, choices = ['CIFAR-100','ILSVRC'], help = 'Training dataset.')
    parser.add_argument('--data_root', type = str, required = True, help = 'Root directory of the dataset.')
    parser.add_argument('--embedding', action = 'append', required = True, help = 'Path to a pickle dump of embeddings generated by compute_class_embeddings.py.')
    parser.add_argument('--architecture', type = str, default = 'simple', choices = utils.ARCHITECTURES, help = 'Type of network architecture.')
    parser.add_argument('--lr_schedule', type = str, default = 'SGDR', choices = utils.LR_SCHEDULES, help = 'Type of learning rate schedule.')
    parser.add_argument('--clipgrad', type = float, default = 10.0, help = 'Gradient norm clipping.')
    parser.add_argument('--epochs', type = int, default = None, help = 'Number of training epochs.')
    parser.add_argument('--batch_size', type = int, default = 100, help = 'Batch size.')
    parser.add_argument('--model_dump', type = str, default = None, help = 'Filename where the learned model should be written to.')
    parser.add_argument('--log_dir', type = str, default = None, help = 'Tensorboard log directory.')
    parser.add_argument('--stop_grad', action = 'store_true', default = False, help = 'If present, overall loss will only be back-propagated to the combination layer, but not to the individual network branches.')
    args = parser.parse_args()

    # Configure environment
    K.set_session(K.tf.Session(config = K.tf.ConfigProto(gpu_options = { 'allow_growth' : True })))

    # Load class embeddings
    embeddings = []
    embed_labels = None
    for path in args.embedding:
        with open(path, 'rb') as pf:
            dump = pickle.load(pf)
            if embed_labels is None:
                embed_labels = dump['ind2label']
            embeddings.append(dump['embedding'])
            del dump

    # Load dataset
    data_generator = get_data_generator(args.dataset, args.data_root, classes = embed_labels)

    # Construct and train model
    reshape_layer = keras.layers.Lambda(lambda x: K.permute_dimensions(K.reshape(x, (-1, 3, 100)), (0, 2, 1)), name = 'reshape')
    stop_grad_layer = keras.layers.Lambda(lambda x: K.stop_gradient(x), name = 'stop_grad')
    
    input_ = keras.layers.Input((32,32,3))
    feat = [utils.build_network(data_generator.num_classes if i == 0 else embeddings[0].shape[1], args.architecture, classification = (i == 0), name = 'b{}'.format(i))(input_) for i in range(1 + len(embeddings))]
    
    if args.stop_grad:
        output_ = [stop_grad_layer(f) for f in feat]
    else:
        output_ = feat
    
    output_ = reshape_layer(keras.layers.concatenate(
        [output_[0]] + [keras.layers.Lambda(lambda x: nearest_centroid_softmax(x, embed), name = 'nearest_centroid_softmax_{}'.format(i))(f) for i, (f, embed) in enumerate(zip(output_[1:], embeddings), start = 1)])
    )
    output_ = keras.layers.Lambda(lambda x: K.exp(x[:,:,0]), name = 'unnorm_comb')(keras.layers.Dense(1, use_bias = False, kernel_constraint = UnitL1Norm() if args.stop_grad else None, name = 'log_comb')(output_))
    output_ = keras.layers.Lambda(lambda x: x / K.sum(x, axis = -1, keepdims = True), name = 'comb')(output_)
    
    combined_net = keras.models.Model(input_, [output_] + feat)
    combined_net.summary()
    
    callbacks, num_epochs = utils.get_lr_schedule(args.lr_schedule, data_generator.num_train, args.batch_size)

    if args.log_dir:
        if os.path.isdir(args.log_dir):
            shutil.rmtree(args.log_dir, ignore_errors = True)
        callbacks.append(keras.callbacks.TensorBoard(log_dir = args.log_dir, write_graph = False))
    
    callbacks.append(PrintWeightsCallback('log_comb'))

    decay = 9 / ((data_generator.num_train // args.batch_size) * (args.epochs if args.epochs else num_epochs))  # lr multiplier should be 0.1 in final epoch
    combined_net.compile(optimizer = keras.optimizers.SGD(lr=0.001, decay=decay, momentum=0.9, clipnorm = args.clipgrad),
                         loss = dict([('comb', 'categorical_crossentropy'), ('b0', 'categorical_crossentropy')] + [('b{}'.format(i), utils.squared_distance) for i in range(1, len(feat))]),
                         loss_weights = dict([('comb', 0.1)] + [('b{}'.format(i), 1.0) for i in range(len(feat))]) if not args.stop_grad else None,
                         metrics = dict([('comb', 'accuracy'), ('b0', 'accuracy')] + [('b{}'.format(i), utils.nn_accuracy(embeddings[i-1])) for i in range(1, len(feat))])
     )

    combined_net.fit_generator(
              gen_inputs(data_generator.flow_train(args.batch_size), embeddings),
              data_generator.num_train // args.batch_size,
              validation_data = gen_inputs(data_generator.flow_test(args.batch_size), embeddings),
              validation_steps = data_generator.num_test // args.batch_size,
              epochs = num_epochs, callbacks = callbacks, verbose = True)

    # Evaluate final performance
    print(combined_net.evaluate_generator(gen_inputs(data_generator.flow_test(args.batch_size), embeddings), data_generator.num_test // args.batch_size))

    # Save model
    if args.model_dump:
        combined_net.save(args.model_dump)
